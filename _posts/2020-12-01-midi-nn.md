---
title: "Neural Network MIDI generater"
date: 2020-12-01
categories:
  - projects
  
tags:
  - music
  - AI
---

An interesting video was brought to my attention while doing some work on other things titled [1500 slot machines walk into a bar](https://www.youtube.com/watch?v=E8Lhqri8tZk). These guys were "hacking" in the purest sense of the term; abusing existing technologies to acheive an end goal.

With this in mind I decided to start looking into how easily I could use a machine to generate music, as the thought of having generated assets for small personal games I may or probably may not develop really interests me.

![assembly](/assets/images/music_nn/739C258B-8451-4605-B79D-A1016C463114.gif)


## HERE COMES SOME NERD SHIT

Its ok, I too dont really care how these work. Im ok with thinking of them as black boxes I throw things into and results spit out. But to put some meat into this post, here is some extremley light theory.

*"Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far."*

Well thats suffecneint for me. But heres some other stuff that irritated me not knowing, so I shall list it.

RNN  
- recurrent neural network. 
- Good for predicting shit. 
- Requires training to learn if its given a pattern of X what the most likely next element in the sequence will be.
- Deceptively simple; accepts an input of x and gives an output of y. This is the whole thing, really.

Hidden layer 
- mathematical function your input of x will be passed through. or have slapped on. I dont rememebr how math works. 
- More hidden layers == more functions being applied.

LSTM
- long short-term memory 
- type of rnn
- Useful when the network has to 'remember' things for a long time, like music or text generation

Steps update the hidden states when called. 1000 steps == 1000 hidden state activations, or the functions being slapped on your input 1000 times.

![makes_sense](/assets/images/music_nn/706074E0-040D-48DB-9D3A-F53F3E091DC5.gif)

And thats about all i care to look into really. [This is an insanely indepth guide, you may enjoy it.](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

So to start, we need python 3 isntalled.

It MUST be 3.7; tensorflow (the one we are using anyway) is not compatable with 3.8.X and this will NOT run under python 2. I cannot say if it works on 3.8.0 or not; i KNOW it runs on 3.7.X however.

you CANNOT install tensorflow with pip install tensorflow as you would expect, you MUST use

````pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl````

next we install magenta, the tool that was made for researching AI in the creative space (music and images). Pretty much a tool purpose built for this experiment!

after installing magenta, our tensorflow is now broken. fix it with
````pip install tensorflow --upgrade --force-reinstall````

now our framework is isntalled. lets get some data to feed our little hungry program!

![hungry_hippo](/assets/images/music_nn/01C0F242-49A8-460A-9357-B15F54F8CA06.gif)


this is a repo of over 1000 midi tracks. it proved a good base for this. I dont recall where I got it from, but you can get it from [upload the midi set somewhere]

now we want to turn the midi tracks into a textual representation of the music. this is a python script from the magenta package that gets isntalled if you did the above.

````convert_dir_to_note_sequences --input_dir="C:\Users\root\desktop\nintendo_midi" --output_file="C:\Users\root\desktop\nintendo_midi.tfrecord" --recursive````

now we create the training set. the ratio is what percentage will be held as evaluation files; ie the nn will learn with 0.9 and the final 0.10 will be what it compares its results with. a helpful, or not, [point is made here](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7) "Like many other things in machine learning, the train-test-validation split ratio is also quite specific to your use case and it gets easier to make judgement as you train and build more and more models." 

Keep this at 0.10 because it was what worked for me, or change it and see what happens. it seems there is a lot of discussion about 70/30 though. i should do a rerun to see if maybe it learns faster with that ratio. There is also a chance that it'll make it learn WORSE and only output garbage, since it's dataset will be reduced.

![sour](/assets/images/music_nn/0D5FABDF-5C66-4F66-B7F0-AC6D7E60B7E4.gif)


````polyphony_rnn_create_dataset --input="C:\Users\root\desktop\nintendo_midi.tfrecord" --output_dir="C:\Users\root\desktop\polyphony_rnn\sequence_examples" --eval_ratio=0.10````


## RUN IT

![running](/assets/images/music_nn/1B83277E-1E2B-4167-A7AD-3789A75144CF.gif)


steps number is how many times itll do work. adjust to vary how long you want it to run. basically one every two minutex on my rig, give or take. i found runs of 1000 to be good to kick off before bed and have it done when i got home from werk.


batch size of 64 instead of the default of 128 to reduce memory usage was what was suggested to me when i started, but realistically i have an inordinate amount so i probably shouldnt have bothered. to make it train faster it was also suggested i use 3 layers of 128 functions, becase i wanted results immediately. i will adjust these to see what happens. maybe itll take a longer time to crunch 1k steps but itll require less steps? hmm.


````polyphony_rnn_train --run_dir="C:\Users\root\desktop\polyphony_rnn\logdir\run3" --sequence_example_file="C:\Users\root\desktop\polyphony_rnn\sequence_examples\training_poly_tracks.tfrecord" --hparams="batch_size=64,rnn_layer_sizes=[128,128,128]" --num_training_steps=1000````


## MAKE SOME MUSIC

![making_music](/assets/images/music_nn/E8278156-4E01-495C-B8B2-081D0E40A40F.gif)

````polyphony_rnn_generate --run_dir="C:\Users\root\Desktop\polyphony_rnn\logdir\run3" --hparams="batch_size=64,rnn_layer_sizes=[128,128,128]" --output_dir="C:\Users\root\Desktop\polyphony_rnn\generated" --num_outputs=10 --num_steps=128 --primer_melody="[20]" --condition_on_primer=true --inject_primer_during_generation=false````



The --primer_melody argument can be replaced with primer_pitches, for example --primer_pitches="[67,64,60]" represents a chord with quarter note duration. --num_steps sets how long the generated track will be; 128 seems to be 30 seconds. 

If the condition_on_primer argument is set to true, then the RNN will receive the primer as its input before it begins generating a new sequence. This is useful if you're using the primer pitches as a chord to establish the key. If inject_primer_during_generation is true, the primer will be injected as a part of the generated sequence. This is useful if you want to harmonize an existing melody.



And voila. After about 8k steps or so it started to sound good. Some of the tracks it generated could easily be shoved into a snes game. I will run these through a vst to have them literally sound like snes tracks, and write up an automated workflow for that cause i think it would be cool.

![simp_zelda](/assets/images/music_nn/455FC722-3E6F-4EC5-9C31-765709D924FC.jpeg)

here is some examples i liked.



